{
    "experiment": {
        "log_dir": null, 
        "hostfile": "",
        "exp_name": "aquila_70b_pretrained"
    },
    "launch": {
        "nnodes": 4,
        "nproc_per_node": 16,
        "node_rank": 0,
        "master_addr": "127.0.0.1",
        "master_port": 54331
    },
    "env_vars": {
        "CUDA_DEVICE_MAX_CONNECTIONS": "1",
        "NCCL_SOCKET_IFNAME": "`ip -4 addr show | grep inet | grep 10.31.12. | sed -e 's/^.*global *//g'`",
        "GLOO_SOCKET_IFNAME": "`ip -4 addr show | grep inet | grep 10.31.12. | sed -e 's/^.*global *//g'`",
        "NCCL_NET_SHARED_BUFFERS": "0",
        "NCCL_FORCESYNC_DISABLE": "1",
        "UMD_CCLINLASTCE": "1",
        "ENABLE_FLASH_ATTENTION_WITH_IXDNN": "1",
        "NCCL_DEBUG": "TRACE",
        "NCCL_ALGO": "Ring",
        "OMP_NUM_THREADS": "4",
        "NCCL_USE_DIRECT": "1"
    },
    "training": {
        "mlp-g1-tp-overlap": true,
        "mlp-g2-tp-overlap": true,
        "attn-g1-tp-overlap": true,
        "attn-g2-tp-overlap": true,
        "mlp-g1-overlap-size": 4,
        "mlp-g2-overlap-size": 4,
        "attn-g1-overlap-size": 4,
        "attn-g2-overlap-size": 4,
        "mlp-g1-save-total-input-for-backward": true,
        "attn-g1-save-total-input-for-backward": true,
        "train_samples": 21972,
        "eval_iters": 0,
        "eval_interval": 2000,
        "tensor_model_parallel_size": 2,
        // 70B 4nodes
        "pipeline_model_parallel_size": 32,
        "micro_batch_size": 1,
        // 70B 7nodes
        // "pipeline_model_parallel_size": 28
        // "micro_batch_size": 2,
        "global_batch_size": 2048,
        "disable_bias_linear": true,
        "sequence_parallel": true,
        "use_flash_attn": true,
        "use-distributed-optimizer": true,
        "no-global-file-system": true,
        "recompute-granularity": "full",
        "recompute-method": "uniform"
    },
    "mixed_precision": {
        "bf16": true,
        "initial-loss-scale": 65536,
        "min-loss-scale": 1.0,
        "loss-scale-window": 1024,
        "embedding_weights_in_fp32": true,
        "attention_softmax_in_fp32": true,
        "accumulate_allreduce_grads_in_fp32": true
    },
    "data": {
        "data_path": [
            "path-to-data"
        ],
        "tokenizer_type": "AquilaTokenizer",
        "vocab_size": 100008,
        "vocab_file": "../examples/aquila/tokenizer/vocab.json",
        "merge_file": "../examples/aquila/tokenizer/merges.txt",
        "special_tokens_file": "../examples/aquila/tokenizer/special_tokens.txt",
        "tokenizer_type": "AquilaTokenizer",
        "data_impl": "mmap",
        "split": "1",
    },
    "network": {
        "num_layers": 80,
        "hidden_size": 8192,
        "num_attention_heads": 64,
        "group_query_attention": true,
        "num_query_groups": 8,
        "hidden_dim_multiplier": 1.3,
        "seq-length": 4096,
        "max_position_embeddings": 4096,
        "layernorm_epsilon": 1e-05,
        "use_rotary_position_embeddings": true,
        "rotary_position_embeddings_in_fp32": true,
        "no_position_embedding": true,
        "swiglu": true,
        "multiple_of": 4096,
        "apply_layernorm_rms": true,
        "make_vocab_size_divisible_by": 64,
        "untie_embeddings_and_output_weights": true,
        "standalone-embedding-stage": true,
        "num-layers-of-first-stage": 1
    },
    "initialization": {
        "seed": 42,
        "init_method_std": 0.02
    },
    "regularization": {
        "attention_dropout": 0.0,
        "hidden_dropout": 0.0,
        "weight_decay": 0.1,
        "adam_beta1": 0.9,
        "adam_beta2": 0.95,
        "clip_grad": 1.0
    },
    "learning_rate": {
        "lr": 1.5e-4,
        "lr_decay_style": "cosine",
        "lr_warmup_samples": 8,
        "min_lr": 1.5e-5
    },
    "logging": {
        "log-interval": 1,
        "tensorboard-dir": "path-to-tensorboard-log-dir",
        "tensorboard_log_interval": 1
    }
}
